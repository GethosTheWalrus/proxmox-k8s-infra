variables:
  REGISTRY_PATH: "git.home:5050/mike/proxmox-k8s-infra" 

stages: 
  - build
  - configure
  - deploy
  - test
  - destroy

# build-python-worker:
#   stage: build
#   tags:
#     - shell
#   variables:
#     SHELL: /bin/bash
#   before_script:
#     - |
#       echo "Current user: $(whoami)"
#       echo "Current directory: $(pwd)"
#       echo "Environment variables:"
#       env | sort
#       source ~/.bashrc || true
#       echo "PATH: $PATH"
#       which docker || echo "Docker not found"
#       docker info || echo "Docker not accessible"
#   script:
#     - cd workers/python
#     - docker build -t ${REGISTRY_PATH}/python-worker:latest .
#     - docker push ${REGISTRY_PATH}/python-worker:latest
#   rules:
#     - if: $CI_COMMIT_BRANCH == "main"
#       when: always
#     - when: manual
#   retry:
#     max: 2
#     when:
#       - runner_system_failure
#       - stuck_or_timeout_failure

# build-typescript-worker:
#   stage: build
#   tags:
#     - shell
#   variables:
#     SHELL: /bin/bash
#   before_script:
#     - |
#       echo "Current user: $(whoami)"
#       echo "Current directory: $(pwd)"
#       echo "Environment variables:"
#       env | sort
#       source ~/.bashrc || true
#       echo "PATH: $PATH"
#       which docker || echo "Docker not found"
#       docker info || echo "Docker not accessible"
#   script:
#     - cd workers/typescript
#     - docker build -t ${REGISTRY_PATH}/typescript-worker:latest .
#     - docker push ${REGISTRY_PATH}/typescript-worker:latest
#   rules:
#     - if: $CI_COMMIT_BRANCH == "main"
#       when: always
#     - when: manual
#   retry:
#     max: 2
#     when:
#       - runner_system_failure
#       - stuck_or_timeout_failure

# build-csharp-worker:
#   stage: build
#   tags:
#     - shell
#   variables:
#     SHELL: /bin/bash
#   before_script:
#     - |
#       echo "Current user: $(whoami)"
#       echo "Current directory: $(pwd)"
#       echo "Environment variables:"
#       env | sort
#       source ~/.bashrc || true
#       echo "PATH: $PATH"
#       which docker || echo "Docker not found"
#       docker info || echo "Docker not accessible"
#   script:
#     - cd workers/csharp
#     - docker build -t ${REGISTRY_PATH}/csharp-worker:latest .
#     - docker push ${REGISTRY_PATH}/csharp-worker:latest
#   rules:
#     - if: $CI_COMMIT_BRANCH == "main"
#       when: always
#     - when: manual
#   retry:
#     max: 2
#     when:
#       - runner_system_failure
#       - stuck_or_timeout_failure

# build-go-worker:
#   stage: build
#   tags:
#     - shell
#   variables:
#     SHELL: /bin/bash
#   before_script:
#     - |
#       echo "Current user: $(whoami)"
#       echo "Current directory: $(pwd)"
#       echo "Environment variables:"
#       env | sort
#       source ~/.bashrc || true
#       echo "PATH: $PATH"
#       which docker || echo "Docker not found"
#       docker info || echo "Docker not accessible"
#   script:
#     - cd workers/go
#     - docker build -t ${REGISTRY_PATH}/go-worker:latest .
#     - docker push ${REGISTRY_PATH}/go-worker:latest
#   rules:
#     - if: $CI_COMMIT_BRANCH == "main"
#       when: always
#     - when: manual
#   retry:
#     max: 2
#     when:
#       - runner_system_failure
#       - stuck_or_timeout_failure

deploy-infra:
  stage: build
  image: 
    name: hashicorp/terraform:latest
    entrypoint: [""]
  before_script:
    - export TF_VAR_username=$(echo "$PVEUSER" | base64 -d)
    - export TF_VAR_password=$(echo "$PVEPASSWORD" | base64 -d)
    - export PROJECT_ID=5
    - export TF_USERNAME=$(echo "$GITLABUSERNAME" | base64 -d)
    - export TF_PASSWORD=$(echo "$GITLABACCESSTOKEN" | base64 -d)
    - export TF_ADDRESS="http://git.home/api/v4/projects/$PROJECT_ID/terraform/state/proxmox-k8s-infra"
    - terraform init -backend-config=address=${TF_ADDRESS} -backend-config=lock_address=${TF_ADDRESS}/lock -backend-config=unlock_address=${TF_ADDRESS}/lock -backend-config=username=${TF_USERNAME} -backend-config=password=${TF_PASSWORD} -backend-config=lock_method=POST -backend-config=unlock_method=DELETE -backend-config=retry_wait_min=5
  script:
    - terraform apply -parallelism=2 --auto-approve=true
    - terraform output vm_private_key > key
    - terraform output vm_public_key > key.pub
    - sed -i '1d;$d' key
    - chmod 600 key
    - cat key
  rules:
  - if: '$CI_COMMIT_BRANCH == "main"'
  artifacts:
    paths:
      - scripts
      - key
      - storage-class.yaml

init-master:
  stage: configure
  image:
    name: ubuntu:latest
  variables:
    K8S1: 192.168.69.80
    ROLE: master
    TOKEN: abcdef.0123456789abcdef
  before_script:
    - apt update && apt install -y openssh-client
  script:
  - ssh -i key -o StrictHostKeyChecking=no k8s@"$K8S1" "sudo ROLE=$ROLE TOKEN=$TOKEN bash -s" < scripts/install-k8s.sh
  - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/hash $CI_PROJECT_DIR/hash
  rules:
  - if: $CI_COMMIT_TITLE =~ /-init$/
    when: always
  allow_failure: false
  needs:
    - deploy-infra
  artifacts:
    paths:
      - hash
      - key
      - scripts

init-workers:
  stage: configure
  image:
    name: ubuntu:latest
  variables:
    K8S1: 192.168.69.80
    K8S2: 192.168.69.81
    K8S3: 192.168.69.82
    K8S4: 192.168.69.83
    JOINTOKEN: abcdef.0123456789abcdef
    ROLE: worker
  before_script:
    - apt update && apt install -y openssh-client
  script:
    - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/hash $CI_PROJECT_DIR/hash
    - ssh -i key -o StrictHostKeyChecking=no k8s@"$K8S2" "sudo ROLE=$ROLE bash -s" < $CI_PROJECT_DIR/scripts/install-k8s.sh $K8S1 $JOINTOKEN $(cat $CI_PROJECT_DIR/hash)
    - ssh -i key -o StrictHostKeyChecking=no k8s@"$K8S3" "sudo ROLE=$ROLE bash -s" < $CI_PROJECT_DIR/scripts/install-k8s.sh $K8S1 $JOINTOKEN $(cat $CI_PROJECT_DIR/hash)
    - ssh -i key -o StrictHostKeyChecking=no k8s@"$K8S4" "sudo ROLE=$ROLE bash -s" < $CI_PROJECT_DIR/scripts/install-k8s.sh $K8S1 $JOINTOKEN $(cat $CI_PROJECT_DIR/hash)
  rules:
  - if: $CI_COMMIT_TITLE =~ /-init$/
    when: always
    allow_failure: false
  needs:
    - deploy-infra
    - init-master

deploy-metallb:
  stage: deploy
  image:
    name: alpine/k8s:1.29.13
  variables:
    K8S1: 192.168.69.80
    METALLB_NAMESPACE: metallb-system
    IP_ADDRESS_RANGE: 192.168.69.95-192.168.69.100
  before_script:
    - apk --no-cache add openssh-client kubectl helm
  script:
    - mkdir ~/.kube
    - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/.kube/config ~/.kube/config
    - export KUBECONFIG=~/.kube/config
    - chmod +x scripts/install-metallb.sh
    - scripts/install-metallb.sh
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_TITLE =~ /-init$/'
      when: always
    - when: manual
  dependencies:
    - init-master
    - init-workers

deploy-nginx:
  stage: deploy
  image:
    name: alpine/k8s:1.29.13
  variables:
    K8S1: 192.168.69.80
  before_script:
    - apk --no-cache add openssh-client
  script:
    - mkdir ~/.kube
    - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/.kube/config ~/.kube/config
    - export KUBECONFIG=~/.kube/config
    - kubectl create deploy nginx --image nginx
    - kubectl expose deploy nginx --port 80 --type LoadBalancer
  rules:
  - when: always
  allow_failure: false
  needs:
    - init-master
    - init-workers
    - deploy-metallb 

deploy-openebs:
  stage: deploy
  image:
    name: alpine/k8s:1.29.13
  variables:
    K8S1: 192.168.69.80
  before_script:
    - apk --no-cache add openssh-client kubectl helm
  script:
    - mkdir ~/.kube
    - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/.kube/config ~/.kube/config
    - export KUBECONFIG=~/.kube/config
    - chmod +x scripts/install-openebs.sh
    - scripts/install-openebs.sh
    - echo "Applying storage class..."
    - kubectl apply -f storage-class.yaml
    - echo "Verifying storage class..."
    - kubectl get sc
    - echo "Verifying OpenEBS pods are running..."
    - kubectl get pods -n openebs
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_TITLE =~ /-init$/'
      when: always
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
  needs:
    - init-master
    - init-workers
    - deploy-metallb

deploy-temporal:
    stage: deploy
    image:
      name: alpine/k8s:1.29.13
    variables:
      K8S1: 192.168.69.80
      LOAD_BALANCER_IP: 192.168.69.98
      WEB_UI_IP: 192.168.69.97
    before_script:
      - apk --no-cache add openssh-client kubectl helm docker-cli
      - mkdir -p /etc/docker
      - |
        cat > /etc/docker/daemon.json << EOF
        {
          "insecure-registries": ["git.home:5050"]
        }
        EOF
    script:
      - mkdir ~/.kube
      - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/.kube/config ~/.kube/config
      - export KUBECONFIG=~/.kube/config
      - |
        if [ -n "$DOCKER_USERNAME" ] && [ -n "$DOCKER_PASSWORD" ]; then
          echo "Attempting to login to Docker Hub..."
          echo "Username: $DOCKER_USERNAME"
          echo "Password length: ${#DOCKER_PASSWORD}"
          docker login docker.io -u $DOCKER_USERNAME -p $DOCKER_PASSWORD || docker login -u $DOCKER_USERNAME -p $DOCKER_PASSWORD
        else
          echo "Docker credentials not provided"
        fi
      - |
        if [ -n "$DOCKER_USERNAME" ] && [ -n "$DOCKER_PASSWORD" ]; then
          echo "Creating Kubernetes secret for Docker Hub..."
          kubectl create secret docker-registry dockerhub-secret --docker-server=docker.io --docker-username=$DOCKER_USERNAME --docker-password=$DOCKER_PASSWORD --docker-email=$DOCKER_EMAIL --namespace=temporal || true
        fi
      - chmod +x scripts/install-temporal.sh
      - scripts/install-temporal.sh
      - echo "=== Temporal Deployment Complete ==="
      - echo "Services:"
      - kubectl get services -n temporal | grep LoadBalancer
      - echo ""
      - echo "Access Information:"
      - echo "  Temporal Frontend (gRPC):${LOAD_BALANCER_IP}:7233"
      - echo "  Temporal Web UI:${WEB_UI_IP}:8080"
    rules:
      - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_TITLE =~ /-init$/'
        when: always
      - if: '$CI_COMMIT_BRANCH == "main"'
        when: manual
    needs:
    - init-master
    - init-workers
    - deploy-metallb
    - deploy-openebs

# test-temporal-workflow:
#   stage: test
#   tags:
#     - shell
#   variables:
#     TEMPORAL_HOST: "192.168.69.98:7233"
#     TEMPORAL_NAMESPACE: "default"
#   before_script:
#     - python3 -m pip install --user temporalio requests
#   script:
#     - |
#       cat > test_workflow.py << 'EOF'
#       import asyncio
#       import os
#       from datetime import timedelta
#       from temporalio.client import Client
#       from temporalio.worker import Worker
#       from temporalio import workflow, activity
#       from temporalio.common import RetryPolicy

#       @workflow.defn
#       class TestWorkflow:
#           @workflow.run
#           async def run(self, name: str) -> str:
#               # Start Python activity
#               python_result = await workflow.execute_activity(
#                   python_activity,
#                   "Hello from Python",
#                   start_to_close_timeout=timedelta(seconds=5),
#                   retry_policy=RetryPolicy(maximum_attempts=3)
#               )
              
#               # Start TypeScript activity
#               typescript_result = await workflow.execute_activity(
#                   "ProcessTypeScript",
#                   "Hello from TypeScript",
#                   start_to_close_timeout=timedelta(seconds=5),
#                   retry_policy=RetryPolicy(maximum_attempts=3)
#               )
              
#               # Start Go activity
#               go_result = await workflow.execute_activity(
#                   "ProcessGo",
#                   "Hello from Go",
#                   start_to_close_timeout=timedelta(seconds=5),
#                   retry_policy=RetryPolicy(maximum_attempts=3)
#               )
              
#               return f"Python: {python_result}, TypeScript: {typescript_result}, Go: {go_result}"

#       @activity.defn
#       async def python_activity(message: str) -> str:
#           return f"Python says: {message}"

#       async def main():
#           # Connect to Temporal server using the LoadBalancer IP
#           client = await Client.connect(
#               os.environ["TEMPORAL_HOST"],
#               namespace=os.environ["TEMPORAL_NAMESPACE"]
#           )
          
#           # Start the workflow
#           handle = await client.start_workflow(
#               TestWorkflow.run,
#               "Temporal",
#               id="test-workflow",
#               task_queue="python-task-queue"
#           )
          
#           # Wait for the result
#           result = await handle.result()
#           print(f"Workflow result: {result}")

#       if __name__ == "__main__":
#           asyncio.run(main())
#       EOF

#     - python3 test_workflow.py
#   rules:
#     - if: $CI_COMMIT_BRANCH == "main"
#       when: always
#     - when: manual
#   environment:
#     name: production
#   needs:
#     - deploy-temporal
#     - deploy-temporal-workers

destroy-cluster:
  stage: destroy
  image: 
    name: hashicorp/terraform:latest
    entrypoint: [""]
  before_script:
    - export TF_VAR_username=$(echo "$PVEUSER" | base64 -d)
    - export TF_VAR_password=$(echo "$PVEPASSWORD" | base64 -d)
    - export PROJECT_ID=5
    - export TF_USERNAME=$(echo "$GITLABUSERNAME" | base64 -d)
    - export TF_PASSWORD=$(echo "$GITLABACCESSTOKEN" | base64 -d)
    - export TF_ADDRESS="http://git.home/api/v4/projects/$PROJECT_ID/terraform/state/proxmox-k8s-infra"
    - terraform init -backend-config=address=${TF_ADDRESS} -backend-config=lock_address=${TF_ADDRESS}/lock -backend-config=unlock_address=${TF_ADDRESS}/lock -backend-config=username=${TF_USERNAME} -backend-config=password=${TF_PASSWORD} -backend-config=lock_method=POST -backend-config=unlock_method=DELETE -backend-config=retry_wait_min=5
  script:
    - terraform destroy --auto-approve=true
  allow_failure: true
  rules:
  - when: manual
  needs: []