variables:
  REGISTRY_PATH: ""

stages:
  - build
  - configure
  - deploy
  - test
  - destroy

deploy-infra:
  stage: build
  image: 
    name: hashicorp/terraform:latest
    entrypoint: [""]
  before_script:
    - export TF_VAR_username=$(echo "$PVEUSER" | base64 -d)
    - export TF_VAR_password=$(echo "$PVEPASSWORD" | base64 -d)
    - export PROJECT_ID=5
    - export TF_USERNAME=$(echo "$GITLABUSERNAME" | base64 -d)
    - export TF_PASSWORD=$(echo "$GITLABACCESSTOKEN" | base64 -d)
    - export TF_ADDRESS="http://git.home/api/v4/projects/$PROJECT_ID/terraform/state/proxmox-k8s-infra"
    - terraform init -backend-config=address=${TF_ADDRESS} -backend-config=lock_address=${TF_ADDRESS}/lock -backend-config=unlock_address=${TF_ADDRESS}/lock -backend-config=username=${TF_USERNAME} -backend-config=password=${TF_PASSWORD} -backend-config=lock_method=POST -backend-config=unlock_method=DELETE -backend-config=retry_wait_min=5
  script:
    - terraform apply -parallelism=2 --auto-approve=true
    - terraform output vm_private_key > key
    - terraform output vm_public_key > key.pub
    - sed -i '1d;$d' key
    - chmod 600 key
    - cat key
  rules:
  - if: '$CI_COMMIT_BRANCH == "main"'
  artifacts:
    paths:
      - scripts
      - key
      - storage-class.yaml

build-python-worker:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  script:
    - docker build -t python-worker:latest workers/python
    - docker save python-worker:latest > python-worker.tar
  artifacts:
    paths:
      - python-worker.tar
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: always
    - when: manual

build-typescript-worker:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  script:
    - docker build -t typescript-worker:latest workers/typescript
    - docker save typescript-worker:latest | gzip -9 > typescript-worker.tar.gz
  artifacts:
    paths:
      - typescript-worker.tar.gz
    expire_in: 1 day
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: always
    - when: manual

build-go-worker:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  script:
    - docker build -t go-worker:latest workers/go
    - docker save go-worker:latest > go-worker.tar
  artifacts:
    paths:
      - go-worker.tar
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: always
    - when: manual

build-csharp-worker:
  stage: build
  image: docker:20.10.16
  services:
    - docker:20.10.16-dind
  script:
    - docker build -t csharp-worker:latest workers/csharp
    - docker save csharp-worker:latest | gzip > csharp-worker.tar.gz
  artifacts:
    paths:
      - csharp-worker.tar.gz
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: always
    - when: manual

init-master:
  stage: configure
  image:
    name: ubuntu:latest
  variables:
    K8S1: 192.168.69.80
    ROLE: master
    TOKEN: abcdef.0123456789abcdef
  before_script:
    - apt update && apt install -y openssh-client
  script:
  - ssh -i key -o StrictHostKeyChecking=no k8s@"$K8S1" "sudo ROLE=$ROLE TOKEN=$TOKEN bash -s" < scripts/install-k8s.sh
  - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/hash $CI_PROJECT_DIR/hash
  rules:
  - if: $CI_COMMIT_TITLE =~ /-init$/
    when: always
  allow_failure: false
  needs:
    - deploy-infra
  artifacts:
    paths:
      - hash
      - key
      - scripts

init-workers:
  stage: configure
  image:
    name: ubuntu:latest
  variables:
    K8S1: 192.168.69.80
    K8S2: 192.168.69.81
    K8S3: 192.168.69.82
    K8S4: 192.168.69.83
    JOINTOKEN: abcdef.0123456789abcdef
    ROLE: worker
  before_script:
    - apt update && apt install -y openssh-client
  script:
    - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/hash $CI_PROJECT_DIR/hash
    - ssh -i key -o StrictHostKeyChecking=no k8s@"$K8S2" "sudo ROLE=$ROLE bash -s" < $CI_PROJECT_DIR/scripts/install-k8s.sh $K8S1 $JOINTOKEN $(cat $CI_PROJECT_DIR/hash)
    - ssh -i key -o StrictHostKeyChecking=no k8s@"$K8S3" "sudo ROLE=$ROLE bash -s" < $CI_PROJECT_DIR/scripts/install-k8s.sh $K8S1 $JOINTOKEN $(cat $CI_PROJECT_DIR/hash)
    - ssh -i key -o StrictHostKeyChecking=no k8s@"$K8S4" "sudo ROLE=$ROLE bash -s" < $CI_PROJECT_DIR/scripts/install-k8s.sh $K8S1 $JOINTOKEN $(cat $CI_PROJECT_DIR/hash)
  rules:
  - if: $CI_COMMIT_TITLE =~ /-init$/
    when: always
    allow_failure: false
  needs:
    - deploy-infra
    - init-master

deploy-metallb:
  stage: deploy
  image:
    name: alpine/k8s:1.29.13
  variables:
    K8S1: 192.168.69.80
    METALLB_NAMESPACE: metallb-system
    IP_ADDRESS_RANGE: 192.168.69.95-192.168.69.100
  before_script:
    - apk --no-cache add openssh-client kubectl helm
  script:
    - mkdir ~/.kube
    - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/.kube/config ~/.kube/config
    - export KUBECONFIG=~/.kube/config
    - chmod +x scripts/install-metallb.sh
    - scripts/install-metallb.sh
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_TITLE =~ /-init$/'
      when: always
    - when: manual
  dependencies:
    - init-master
    - init-workers

deploy-nginx:
  stage: deploy
  image:
    name: alpine/k8s:1.29.13
  variables:
    K8S1: 192.168.69.80
  before_script:
    - apk --no-cache add openssh-client
  script:
    - mkdir ~/.kube
    - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/.kube/config ~/.kube/config
    - export KUBECONFIG=~/.kube/config
    - kubectl create deploy nginx --image nginx
    - kubectl expose deploy nginx --port 80 --type LoadBalancer
  rules:
  - when: always
  allow_failure: false
  needs:
    - init-master
    - init-workers
    - deploy-metallb

deploy-openebs:
  stage: deploy
  image:
    name: alpine/k8s:1.29.13
  variables:
    K8S1: 192.168.69.80
  before_script:
    - apk --no-cache add openssh-client kubectl helm
  script:
    - mkdir ~/.kube
    - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/.kube/config ~/.kube/config
    - export KUBECONFIG=~/.kube/config
    - chmod +x scripts/install-openebs.sh
    - scripts/install-openebs.sh
    - echo "Applying storage class..."
    - kubectl apply -f storage-class.yaml
    - echo "Verifying storage class..."
    - kubectl get sc
    - echo "Verifying OpenEBS pods are running..."
    - kubectl get pods -n openebs
  rules:
    - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_TITLE =~ /-init$/'
      when: always
    - if: '$CI_COMMIT_BRANCH == "main"'
      when: manual
  needs:
    - init-master
    - init-workers
    - deploy-metallb

deploy-temporal:
    stage: deploy
    image:
      name: alpine/k8s:1.29.13
    variables:
      K8S1: 192.168.69.80
      LOAD_BALANCER_IP: 192.168.69.98
    before_script:
      - apk --no-cache add openssh-client kubectl helm docker-cli
      # Configure Docker to use insecure registry
      - mkdir -p /etc/docker
      - |
        cat > /etc/docker/daemon.json << EOF
        {
          "insecure-registries": ["git.home:5050"]
        }
        EOF
      # Login to Docker Hub if credentials are provided
      - |
        if [ -n "$DOCKER_USERNAME" ] && [ -n "$DOCKER_PASSWORD" ]; then
          echo "Attempting to login to Docker Hub..."
          echo "Username: $DOCKER_USERNAME"
          echo "Password length: ${#DOCKER_PASSWORD}"
          # Try login with explicit registry
          docker login docker.io -u $DOCKER_USERNAME -p $DOCKER_PASSWORD
          # If that fails, try without specifying registry
          if [ $? -ne 0 ]; then
            echo "First login attempt failed, trying alternative method..."
            docker login -u $DOCKER_USERNAME -p $DOCKER_PASSWORD
          fi
        else
          echo "Docker credentials not provided"
          echo "DOCKER_USERNAME is set: $([ -n "$DOCKER_USERNAME" ] && echo 'yes' || echo 'no')"
          echo "DOCKER_PASSWORD is set: $([ -n "$DOCKER_PASSWORD" ] && echo 'yes' || echo 'no')"
        fi 
      # Create Kubernetes secret for Docker Hub credentials
      - |
        if [ -n "$DOCKER_USERNAME" ] && [ -n "$DOCKER_PASSWORD" ]; then
          echo "Creating Kubernetes secret for Docker Hub..."
          kubectl create secret docker-registry dockerhub-secret \
            --docker-server=docker.io \
            --docker-username=$DOCKER_USERNAME \
            --docker-password=$DOCKER_PASSWORD \
            --docker-email=$DOCKER_EMAIL \
            --namespace=temporal || true
        fi
    script:
      - mkdir ~/.kube
      - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/.kube/config ~/.kube/config
      - export KUBECONFIG=~/.kube/config
      - chmod +x scripts/install-temporal.sh
      - scripts/install-temporal.sh
    rules:
      - if: '$CI_COMMIT_BRANCH == "main" && $CI_COMMIT_TITLE =~ /-init$/'
        when: always
      - if: '$CI_COMMIT_BRANCH == "main"'
        when: manual
    needs:
    - init-master
    - init-workers
    - deploy-metallb
    - deploy-openebs

deploy-temporal-workers:
  stage: deploy
  image:
    name: alpine/k8s:1.29.13
  variables:
    K8S1: 192.168.69.80
  before_script:
    - apk --no-cache add openssh-client kubectl helm docker-cli docker
  script:
    - mkdir ~/.kube
    - scp -i key -o StrictHostKeyChecking=no k8s@"$K8S1":~/.kube/config ~/.kube/config
    - export KUBECONFIG=~/.kube/config

    # Load the Docker images on the K8s nodes using ssh to execute docker commands on the nodes
    - |
      for node in $K8S1 192.168.69.81 192.168.69.82 192.168.69.83; do
        echo "Loading images on node $node..."
        scp -i key -o StrictHostKeyChecking=no python-worker.tar typescript-worker.tar.gz csharp-worker.tar.gz go-worker.tar k8s@"$node":~/
        ssh -i key -o StrictHostKeyChecking=no k8s@"$node" "docker load < python-worker.tar"
        ssh -i key -o StrictHostKeyChecking=no k8s@"$node" "gunzip -c typescript-worker.tar.gz | docker load"
        ssh -i key -o StrictHostKeyChecking=no k8s@"$node" "gunzip -c csharp-worker.tar.gz | docker load"
        ssh -i key -o StrictHostKeyChecking=no k8s@"$node" "docker load < go-worker.tar"
      done

    # Update the worker YAML files to use local images
    - |
      cat > workers/k8s/python-worker.yaml << EOF
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: python-worker
        namespace: temporal
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: python-worker
        template:
          metadata:
            labels:
              app: python-worker
          spec:
            containers:
            - name: worker
              image: python-worker:latest
              imagePullPolicy: Never
      EOF

    - |
      cat > workers/k8s/typescript-worker.yaml << EOF
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: typescript-worker
        namespace: temporal
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: typescript-worker
        template:
          metadata:
            labels:
              app: typescript-worker
          spec:
            containers:
            - name: worker
              image: typescript-worker:latest
              imagePullPolicy: Never
      EOF

    - |
      cat > workers/k8s/csharp-worker.yaml << EOF
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: csharp-worker
        namespace: temporal
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: csharp-worker
        template:
          metadata:
            labels:
              app: csharp-worker
          spec:
            containers:
            - name: worker
              image: csharp-worker:latest
              imagePullPolicy: Never
      EOF

    - |
      cat > workers/k8s/go-worker.yaml << EOF
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: go-worker
        namespace: temporal
      spec:
        replicas: 1
        selector:
          matchLabels:
            app: go-worker
        template:
          metadata:
            labels:
              app: go-worker
          spec:
            containers:
            - name: worker
              image: go-worker:latest
              imagePullPolicy: Never
      EOF

    # Deploy the workers
    - kubectl apply -f workers/k8s/python-worker.yaml
    - kubectl apply -f workers/k8s/typescript-worker.yaml
    - kubectl apply -f workers/k8s/csharp-worker.yaml
    - kubectl apply -f workers/k8s/go-worker.yaml

    # Debug pod status
    - echo "Checking pod status..."
    - kubectl get pods -n temporal
    - echo "Checking pod events..."
    - kubectl get events -n temporal --sort-by='.lastTimestamp'

    # Wait for deployments with timeout and debug
    - |
      for deployment in python-worker typescript-worker csharp-worker go-worker; do
        echo "Waiting for $deployment deployment..."
        kubectl rollout status deployment/$deployment -n temporal --timeout=300s || {
          echo "Deployment $deployment failed to roll out. Checking pod status..."
          kubectl get pods -n temporal -l app=$deployment
          kubectl describe pods -n temporal -l app=$deployment
          exit 1
        }
      done
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: always
    - when: manual
  environment:
    name: production
  needs:
    - build-python-worker
    - build-typescript-worker
    - build-csharp-worker
    - build-go-worker
    - init-master
    - init-workers
    - deploy-metallb
    - deploy-openebs
    - deploy-temporal

test-temporal-workflow:
  stage: test
  image: python:3.9
  variables:
    TEMPORAL_HOST: "192.168.69.98:7233"
    TEMPORAL_NAMESPACE: "default"
  script:
    - pip install temporalio
    - pip install requests
    - |
      cat > test_workflow.py << 'EOF'
      import asyncio
      from datetime import timedelta
      from temporalio.client import Client
      from temporalio.worker import Worker
      from temporalio import workflow, activity
      from temporalio.common import RetryPolicy

      @workflow.defn
      class TestWorkflow:
          @workflow.run
          async def run(self, name: str) -> str:
              # Start Python activity
              python_result = await workflow.execute_activity(
                  python_activity,
                  "Hello from Python",
                  start_to_close_timeout=timedelta(seconds=5),
                  retry_policy=RetryPolicy(maximum_attempts=3)
              )
              
              # Start TypeScript activity
              typescript_result = await workflow.execute_activity(
                  "ProcessTypeScript",
                  "Hello from TypeScript",
                  start_to_close_timeout=timedelta(seconds=5),
                  retry_policy=RetryPolicy(maximum_attempts=3)
              )
              
              # Start Go activity
              go_result = await workflow.execute_activity(
                  "ProcessGo",
                  "Hello from Go",
                  start_to_close_timeout=timedelta(seconds=5),
                  retry_policy=RetryPolicy(maximum_attempts=3)
              )
              
              return f"Python: {python_result}, TypeScript: {typescript_result}, Go: {go_result}"

      @activity.defn
      async def python_activity(message: str) -> str:
          return f"Python says: {message}"

      async def main():
          # Use the environment variable for Temporal address
          client = await Client.connect(os.environ["TEMPORAL_HOST"], namespace=os.environ["TEMPORAL_NAMESPACE"])
          
          # Start the workflow
          handle = await client.start_workflow(
              TestWorkflow.run,
              "Temporal",
              id="test-workflow",
              task_queue="python-task-queue"
          )
          
          # Wait for the result
          result = await handle.result()
          print(f"Workflow result: {result}")

      if __name__ == "__main__":
          import os
          asyncio.run(main())
      EOF
    - python test_workflow.py
  rules:
    - if: $CI_COMMIT_BRANCH == "main"
      when: always
    - when: manual
  environment:
    name: production
  needs:
    - deploy-temporal
    - deploy-temporal-workers

destroy-cluster:
  stage: destroy
  image: 
    name: hashicorp/terraform:latest
    entrypoint: [""]
  before_script:
    - export TF_VAR_username=$(echo "$PVEUSER" | base64 -d)
    - export TF_VAR_password=$(echo "$PVEPASSWORD" | base64 -d)
    - export PROJECT_ID=5
    - export TF_USERNAME=$(echo "$GITLABUSERNAME" | base64 -d)
    - export TF_PASSWORD=$(echo "$GITLABACCESSTOKEN" | base64 -d)
    - export TF_ADDRESS="http://git.home/api/v4/projects/$PROJECT_ID/terraform/state/proxmox-k8s-infra"
    - terraform init -backend-config=address=${TF_ADDRESS} -backend-config=lock_address=${TF_ADDRESS}/lock -backend-config=unlock_address=${TF_ADDRESS}/lock -backend-config=username=${TF_USERNAME} -backend-config=password=${TF_PASSWORD} -backend-config=lock_method=POST -backend-config=unlock_method=DELETE -backend-config=retry_wait_min=5
  script:
    - terraform destroy --auto-approve=true
  allow_failure: true
  rules:
  - when: manual
  needs: []